"""
Loss Scale Comparison Script

Compares model performance across different data scales (10k, 100k, 1M).
Useful for analyzing how model performance improves with more training data.

Reads losses.parquet files generated by analyze.py.

Usage:
    python scripts/loss_scale.py --model TraONet
    python scripts/loss_scale.py --model MambONet
"""

import fireducks.pandas as fd
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import scienceplots
import os
import beaupy
from rich.console import Console
from typing import List
import argparse


console = Console()


def choose_projects_to_plot():
    project_names = []

    # List all folders in figs
    if not os.path.exists("figs"):
        console.print("[red]Error: figs directory not found[/red]")
        return []

    for d in os.listdir("figs"):
        if os.path.isdir(os.path.join("figs", d)):
            # Check if losses.parquet exists in this folder
            if os.path.exists(os.path.join("figs", d, "losses.parquet")):
                project_names.append(os.path.basename(d))

    if not project_names:
        console.print("[yellow]No projects with losses.parquet found[/yellow]")
        return []

    # Sort the project names
    project_names.sort()

    console.print("Choose projects to compare (select in order: 10k, 100k, 1M):")
    selected_projects = beaupy.select_multiple(project_names)

    selected_projects = [
        os.path.join("figs", d) for d in selected_projects
    ]  # pyright:ignore

    return selected_projects


def losses_from_projects(projects: List[str]):
    if len(projects) < 1:
        return []

    losses = []

    # Get solver losses from first project (they're the same across all)
    df = fd.read_parquet(os.path.join(projects[0], "losses.parquet"))
    if "y4_loss" in df.columns:
        losses.append(df["y4_loss"].to_numpy())
    if "rk4_loss" in df.columns:
        losses.append(df["rk4_loss"].to_numpy())

    # Get model losses from each project
    for project in projects:
        df = fd.read_parquet(os.path.join(project, "losses.parquet"))
        if "model_loss" in df.columns:
            losses.append(df["model_loss"].to_numpy())

    return losses


def hist_losses(losses: List[np.ndarray], legends: List[str], output_name: str = "loss_hist"):
    if len(losses) == 0:
        console.print("[yellow]No losses to plot[/yellow]")
        return

    bins = np.logspace(-12, -2, 100, base=10)
    colors = ["gray", "orange", "darkred", "darkgreen", "darkblue", "purple", "cyan"]

    with plt.style.context(["science", "nature"]):
        fig, ax = plt.subplots()
        loss_means = []
        loss_stds = []
        loss_geo_means = []
        loss_log_stds = []
        loss_medians = []
        loss_q1s = []
        loss_q3s = []
        loss_iqrs = []

        for i, (loss, legend) in enumerate(zip(losses, legends)):
            loss_mean = np.mean(loss)
            loss_std = np.std(loss)
            loss_geo_mean = stats.gmean(loss[loss > 0])
            loss_log_std = np.std(np.log10(loss[loss > 0]))
            loss_med = np.median(loss)
            loss_q1, loss_q3 = np.percentile(loss, [25, 75])
            iqr = loss_q3 - loss_q1

            loss_means.append(loss_mean)
            loss_stds.append(loss_std)
            loss_geo_means.append(loss_geo_mean)
            loss_log_stds.append(loss_log_std)
            loss_medians.append(loss_med)
            loss_q1s.append(loss_q1)
            loss_q3s.append(loss_q3)
            loss_iqrs.append(iqr)

            color_idx = i % len(colors)
            ax.hist(
                loss,
                bins=bins,
                label=legend,
                color=colors[color_idx],
                histtype="step",
                alpha=0.65,
            )

        ax.set_xlabel("Test Loss")
        ax.set_ylabel("Count")
        ax.set_xscale("log")
        ax.set_xlim((1e-12, 1e-2))
        ax.legend(fontsize=6)
        fig.tight_layout()
        fig.savefig(f"figs/{output_name}.png", dpi=600, bbox_inches="tight")
        plt.close(fig)

        # Print statistics table
        df_stats = pd.DataFrame(
            {
                "Legend": legends,
                "Mean": loss_means,
                "Std": loss_stds,
                "Geo Mean": loss_geo_means,
                "Log Std": loss_log_stds,
                "Median": loss_medians,
                "Q1": loss_q1s,
                "Q3": loss_q3s,
                "IQR": loss_iqrs,
            }
        )
        pd.set_option("display.float_format", lambda x: "%.4e" % x)
        console.print("\n[bold]Loss Statistics:[/bold]")
        print(df_stats.to_string(index=False))


def hist_losses_scale_comparison(
    losses: List[np.ndarray],
    legends: List[str],
    model_name: str,
    output_name: str = "loss_hist_scale"
):
    """
    Plot model losses across different data scales.
    Expects losses[0], losses[1] to be Y4, RK4 (solvers)
    and losses[2:] to be model losses at different scales.
    """
    if len(losses) < 3:
        console.print("[yellow]Not enough losses for scale comparison[/yellow]")
        return

    # Skip first two (Y4, RK4) - use only model losses
    model_losses = losses[2:]
    model_legends = legends[2:] if len(legends) > 2 else [f"{model_name} ({i})" for i in range(len(model_losses))]

    if len(model_losses) == 0:
        return

    # Calculate dynamic bins based on data
    all_losses = np.concatenate(model_losses)
    min_loss = max(all_losses.min(), 1e-12)
    max_loss = min(all_losses.max(), 1e-2)
    bins = np.logspace(np.log10(min_loss) * 1.01, np.log10(max_loss) * 0.99, 100)

    colors = ["darkred", "darkgreen", "darkblue", "purple", "cyan"]

    with plt.style.context(["science", "nature"]):
        fig, ax = plt.subplots()

        for i, (loss, legend) in enumerate(zip(model_losses, model_legends)):
            color_idx = i % len(colors)
            ax.hist(
                loss,
                bins=bins,
                label=legend,
                color=colors[color_idx],
                histtype="step",
                alpha=0.65,
                linewidth=1.2,
            )

        ax.set_xlabel("Test Loss")
        ax.set_ylabel("Count")
        ax.set_xscale("log")
        ax.legend(fontsize=6)
        fig.tight_layout()
        fig.savefig(f"figs/{output_name}.png", dpi=600, bbox_inches="tight")
        plt.close(fig)

        console.print(f"[green]Saved scale comparison to: figs/{output_name}.png[/green]")

        # Print improvement statistics
        if len(model_losses) >= 2:
            console.print(f"\n[bold]{model_name} Scaling Analysis:[/bold]")
            for i in range(len(model_losses)):
                geo_mean = stats.gmean(model_losses[i][model_losses[i] > 0])
                median = np.median(model_losses[i])
                console.print(f"  {model_legends[i]}: Geo Mean = {geo_mean:.4e}, Median = {median:.4e}")

            if len(model_losses) >= 2:
                improvement = stats.gmean(model_losses[0]) / stats.gmean(model_losses[-1])
                console.print(f"\n  [cyan]Improvement (first vs last): {improvement:.2f}x[/cyan]")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Plot loss scale comparison")
    parser.add_argument("--model", type=str, default="Model", help="Model name for legend")
    args = parser.parse_args()

    console.print(f"[bold]Loss Scale Comparison for {args.model}[/bold]")

    selected = choose_projects_to_plot()

    if not selected:
        console.print("[red]No projects selected, exiting[/red]")
        exit(1)

    losses = losses_from_projects(selected)

    if not losses:
        console.print("[red]No losses loaded, exiting[/red]")
        exit(1)

    # Create legends based on selection order
    # Assume user selects in order: 10k, 100k, 1M (or similar scales)
    n_solvers = 2
    n_models = len(losses) - n_solvers

    solver_legends = ["Y4", "RK4"][:min(n_solvers, len(losses))]

    # Try to infer scale from project names, otherwise use generic labels
    scale_labels = []
    for p in selected:
        name = os.path.basename(p).lower()
        if "10k" in name or "normal" in name:
            scale_labels.append(f"{args.model} (10k)")
        elif "100k" in name or "more" in name:
            scale_labels.append(f"{args.model} (100k)")
        elif "1m" in name or "much" in name:
            scale_labels.append(f"{args.model} (1M)")
        else:
            scale_labels.append(f"{args.model} ({os.path.basename(p)})")

    legends = solver_legends + scale_labels

    console.print(f"\n[bold]Comparing {len(losses)} loss distributions:[/bold]")
    for i, legend in enumerate(legends):
        console.print(f"  {i+1}. {legend}")

    hist_losses(losses, legends, output_name="loss_hist_scale_all")
    hist_losses_scale_comparison(losses, legends, args.model, output_name="loss_hist_scale")
